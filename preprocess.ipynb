{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "tmp_dir = 'cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中共有数据8210个\n",
      "验证集中共有数据10551个\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_table('data/data_train_image.txt', header=None, sep=' ', na_filter=False)\n",
    "train.columns = ['id', 'label', 'url']\n",
    "val = pd.read_table('data/val.txt', header=None, sep=' ', na_filter=False)\n",
    "val.columns = ['id', 'label', 'url']\n",
    "print(u'训练集中共有数据%d个' % train.shape[0])\n",
    "print(u'验证集中共有数据%d个' % val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集中共有数据18761个, 其中有重复数据75个\n"
     ]
    }
   ],
   "source": [
    "# 将train和val合并后，添加\"num\"列，表示对应id出现了几次，方便后面对重复id进行处理\n",
    "data = pd.concat([train, val])\n",
    "data = data.sort_values(by = ['id'])\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.reset_index()\n",
    "\n",
    "data2 = data[['id', 'index']].groupby(['id'], as_index=False).min()\n",
    "data = pd.merge(data, data2, on='id', how='left')\n",
    "data['num'] = data['index_x'] - data['index_y'] + 1\n",
    "\n",
    "data = data[['id', 'label', 'num']]\n",
    "data = data.sort_values(by = ['label'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "repeated_ids = data[data.num > 1]['id']\n",
    "print(u'数据集中共有数据%d个, 其中有重复数据%d个' % (data.shape[0], repeated_ids.shape[0]))\n",
    "repeated_df = data[data.id.isin(repeated_ids.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除重复图片75张\n"
     ]
    }
   ],
   "source": [
    "# 对于重复id，有两个不同的label，取第一个label，并将重复数据的信息存入repeated_df.pkl文件\n",
    "# 包括其id和两个label值\n",
    "label1 = repeated_df[repeated_df.num == 1]\n",
    "label2 = repeated_df[repeated_df.num == 2]\n",
    "repeated_df = pd.merge(label1[['id', 'label']], label2[['id', 'label']], on='id', how='left')\n",
    "with open(tmp_dir + 'repeated_df.pkl', 'wb') as f:\n",
    "    pickle.dump(repeated_df, f)\n",
    "\n",
    "data = data[data.num == 1]\n",
    "del data['num']\n",
    "print(u'删除重复图片%d张' % repeated_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmrf_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/test2/test'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将图片路径整理，方便keras直接读取\n",
    "rmrf_mkdir('data/train2')\n",
    "dirs = set(data.label)\n",
    "for dirname in dirs:\n",
    "    os.mkdir('data/train2/' + str(dirname))\n",
    "for i, x in data[['id', 'label']].iterrows():\n",
    "    shutil.copyfile('data/alldata/' + x.id + '.jpg', 'data/train2/' + str(x.label) + '/' + x.id + '.jpg')\n",
    "\n",
    "rmrf_mkdir('data/test2')\n",
    "shutil.copytree('data/test', 'data/test2/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导出特征向量\n",
    "def write_gap(MODEL, model_name, image_size, lambda_func=None):\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor = x, weights = 'imagenet', include_top = False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "\n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory('data/train2', image_size, shuffle = False, batch_size = 16)\n",
    "    test_generator = gen.flow_from_directory('data/test2', image_size, shuffle = False, batch_size = 16, class_mode = None)\n",
    "\n",
    "    if os.path.exists(tmp_dir + 'data_info.pkl'):\n",
    "        print(u'data_info.pkl文件已存在')\n",
    "    else:\n",
    "        train_labels = []\n",
    "        train_ids = []\n",
    "        for fname in train_generator.filenames:\n",
    "            train_label = int(fname[:fname.rfind('/')])\n",
    "            train_id = str(fname[fname.rfind('/') + 1 : fname.rfind('.')])\n",
    "            train_labels.append(train_label)\n",
    "            train_ids.append(train_id)\n",
    "        train_info = pd.DataFrame({'id': train_ids, 'label': train_labels})\n",
    "        \n",
    "        test_ids = []\n",
    "        for fname in test_generator.filenames:\n",
    "            test_id = str(fname[fname.rfind('/') + 1 : fname.rfind('.')])\n",
    "            test_ids.append(test_id)\n",
    "        test_info = pd.Series({'id': test_ids})\n",
    "        \n",
    "        label_dict = train_generator.class_indices\n",
    "        label_dict = {int(key): value for key, value in label_dict.items()}\n",
    "        \n",
    "        data_info = {'train': train_info, 'test': test_info, 'label': label_dict}\n",
    "        with open(tmp_dir + 'data_info.pkl', 'wb') as f:\n",
    "            pickle.dump(data_info, f)\n",
    "        print(u'data_info.pkl文件写入成功')\n",
    "    \n",
    "    train = model.predict_generator(train_generator, train_generator.samples/16)\n",
    "    test = model.predict_generator(test_generator, test_generator.samples/16)\n",
    "\n",
    "    with h5py.File(tmp_dir + 'gap_%s.h5' % model_name) as h:\n",
    "        h.create_dataset('train', data = train)\n",
    "        h.create_dataset('test', data = test)\n",
    "        h.create_dataset('label', data = train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_gap(ResNet50, 'ResNet50', (224, 224))\n",
    "write_gap(InceptionV3, 'InceptionV3', (299, 299), inception_v3.preprocess_input)\n",
    "write_gap(Xception, 'Xception', (299, 299), xception.preprocess_input)\n",
    "write_gap(VGG16, 'VGG16', (224, 224))\n",
    "write_gap(VGG19, 'VGG19', (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
