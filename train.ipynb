{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/maqiao/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "\n",
    "tmp_dir = 'cache/'\n",
    "np.random.seed = 2017\n",
    "SEED = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 载入特征向量\n",
    "def load_data():\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    for filename in ['gap_ResNet50.h5', 'gap_InceptionV3.h5', 'gap_Xception.h5']:\n",
    "        with h5py.File(tmp_dir + filename, 'r') as h:\n",
    "            X_train.append(np.array(h['train']))\n",
    "            X_test.append(np.array(h['test']))\n",
    "            y_train = np.array(h['label'])\n",
    "    X_train = np.concatenate(X_train, axis=1)\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "    return(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_info():\n",
    "    with open(tmp_dir + 'data_info.pkl', 'rb') as f:\n",
    "        data_info = pickle.load(f)\n",
    "    train_info = data_info['train']\n",
    "    test_info = data_info['test']\n",
    "    label_dict = data_info['label']\n",
    "    label_dict_reverse = {value: key for key, value in label_dict.items()}\n",
    "    return(train_info, test_info, label_dict, label_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据预处理函数，对神经网络和xgboost分别将whether_to_categorical设置为True和False\n",
    "def preproc(X_train, train_df, label_dict, whether_to_categorical=True):\n",
    "    df = train_df.copy()\n",
    "    df['label'] = train_df['label'].map(label_dict)\n",
    "    if whether_to_categorical:\n",
    "        y_train = np_utils.to_categorical(df.label)\n",
    "    else:\n",
    "        y_train = df.label.values\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    return(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 全连接层预测函数\n",
    "def cnn_predict(X_train, y_train, X_test, epochs=45):\n",
    "    input_tensor = Input(X_train.shape[1:])\n",
    "    x = Dropout(0.7)(input_tensor)\n",
    "    x = Dense(100, activation = 'sigmoid')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "    model.compile(optimizer=RMSprop(lr = 1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=256, epochs=epochs, validation_split=0.2)\n",
    "    preds = model.predict(X_test, verbose=1)\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgboost预测函数，参数待优化\n",
    "def xgb_predict(X_train, y_train, X_test):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=SEED)\n",
    "    dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "    dtest = xgb.DMatrix(X_val, label = y_val)\n",
    "    param = {'learning_rate': 0.1, 'n_estimators': 300, 'max_depth': 5,\n",
    "             'min_child_weight': 5, 'gamma': 1, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "             'scale_pos_weight': 1, 'eta': 0.05, 'silent': 1, 'objective': 'multi:softprob', 'num_class': 100}\n",
    "    num_round = 100\n",
    "    plst = list(param.items())\n",
    "    evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    bst = xgb.train(plst, dtrain, num_round, evallist)\n",
    "        \n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    preds = bst.predict(dtest).reshape(X_test.shape[0], 100)\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_predict(X_train, y_train, X_test):\n",
    "    clf = svm.SVC(decision_function_shape='ovr')\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.decision_function(X_test)\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对预测结果进行后处理，将label映射回134类\n",
    "def postproc(preds, label_dict):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    preds = pd.Series(preds)\n",
    "    label_dict_reverse = {value: key for key, value in label_dict.items()}\n",
    "    preds = preds.map(label_dict_reverse)\n",
    "    return(preds.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 移除数据函数\n",
    "def remove_ids(X_train, train_df, rm_ids):\n",
    "    \"\"\"移除id在rm_ids列表中的数据\"\"\"\n",
    "    remains = (1 - train_df.id.isin(rm_ids)).astype(bool).values\n",
    "    X_remains = X_train[remains]\n",
    "    remains_df = train_df[remains]\n",
    "    return(X_remains, remains_df)\n",
    "\n",
    "# label修改函数\n",
    "def modifi_labels(train_df, modifi_df):\n",
    "    \"\"\"对train_df中的部分label进行修改，以modifi_df中的label为准\"\"\"\n",
    "    df = pd.merge(train_df, modifi_df, on='id', how='inner')\n",
    "    df = df[df.label_x != df.label_y]\n",
    "    train = train_df.set_index(['id'])\n",
    "    for i, row in df.iterrows():\n",
    "        train.loc[row.id, 'label'] = row.label_y\n",
    "    train = train.reset_index()\n",
    "    return(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对repeated_df中重复的数据进行预测，将预测结果存入prediction_of_repeated.pkl文件\n",
    "# 若预测值与之前的已有的两个label都不相同，删除之；若与其中一个相同，修改为预测值\n",
    "def modifi_repeated(X_data, data_info, label_dict, epochs = 45):\n",
    "    dump_file = tmp_dir + 'prediction_of_repeated.pkl'\n",
    "    if os.path.exists(dump_file):\n",
    "        with open(dump_file, 'rb') as f:\n",
    "            pred_df = pickle.load(f)\n",
    "    else:\n",
    "        repeated_file = 'cache/repeated_df.pkl'\n",
    "        with open(repeated_file, 'rb') as f:\n",
    "            repeated = pickle.load(f)\n",
    "        remains = data_info.id.isin(repeated.id.values)\n",
    "        X_test = X_data[remains]\n",
    "        test_info = data_info[remains]\n",
    "        X_train, train_info = remove_ids(X_data, data_info, repeated.id.values)\n",
    "        X_train, y_train = preproc(X_train, train_info, label_dict)\n",
    "        y_preds = cnn_predict(X_train, y_train, X_test, epochs)\n",
    "        test_info['label'] = postproc(y_preds, label_dict)\n",
    "        pred_df = pd.merge(test_info, repeated, on='id', how='left')\n",
    "        with open(dump_file, 'wb') as f:\n",
    "            pickle.dump(pred_df, f)\n",
    "    \n",
    "    rm_ids = pred_df[(pred_df.label != pred_df.label_x) & (pred_df.label != pred_df.label_y)]['id']\n",
    "    modifi_df = pred_df[(pred_df.label == pred_df.label_x) | (pred_df.label == pred_df.label_y)]\n",
    "    X_data, data_info = remove_ids(X_data, data_info, rm_ids.values)\n",
    "    data_info = modifi_labels(data_info, modifi_df[['id', 'label']])\n",
    "    return(X_data, data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对所有数据进行交叉预测，返回预测概率\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "def cross_predict(n_splits = 10, epochs=70):\n",
    "    dump_file = tmp_dir + 'cross_predict.pkl'\n",
    "    if os.path.exists(dump_file):\n",
    "        with open(dump_file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "    else:\n",
    "        train_info, test_info, label_dict, label_dict_reverse = load_info()\n",
    "        X_train, y_train, X_test = load_data()\n",
    "        X, y_df = shuffle(X_train, train_info)\n",
    "        kf = KFold(n_splits = n_splits)\n",
    "        y_preds = []\n",
    "        for train, test in kf.split(X):\n",
    "            print(u'\\n第%d次交叉预测' % (len(y_preds) + 1))\n",
    "            X_train, X_test, y_train_df, y_test_df = X[train], X[test], y_df.iloc[train,:], y_df.iloc[test,:]\n",
    "            X_train, y_train = preproc(X_train, y_train_df, label_dict)\n",
    "            y_pred = cnn_predict(X_train, y_train, X_test, epochs)\n",
    "            y_preds.append(y_pred)\n",
    "        y_preds = np.concatenate(y_preds)\n",
    "    \n",
    "        preds = []\n",
    "        max_probas = []\n",
    "        for ys in y_preds:\n",
    "            pred = np.argmax(ys)\n",
    "            total = sum(ys)\n",
    "            ys = ys / total\n",
    "            max_proba = max(ys)\n",
    "            preds.append(pred)\n",
    "            max_probas.append(max_proba)\n",
    "        df = y_df.copy()\n",
    "        df['pred'] = preds\n",
    "        df['max_proba'] = max_probas\n",
    "        df['pred'] = df['pred'].map(label_dict_reverse).astype(int)\n",
    "    \n",
    "        wrong_rate = df[df.label != df.pred].shape[0] / df.shape[0]\n",
    "        print(u'%d折交叉预测错误率: %f' % (n_splits, wrong_rate))\n",
    "        with open(dump_file, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 依据cross_predict函数预测的概率\n",
    "# 若概率大于modifi_point，说明该数据可能标注错误，修改为预测的lable\n",
    "# 若概率小于rm_point, 可能不是狗的图片或狗的特征不明显的图片，删除它们\n",
    "def remove_and_modifi(X_train, train_info, modifi_point = 0.9, rm_point = 0.5):\n",
    "    cv_df = cross_predict()\n",
    "    modifi_df = cv_df[(cv_df.label != cv_df.pred) & (cv_df.max_proba > modifi_point)]\n",
    "    rm_ids = cv_df[(cv_df.label != cv_df.pred) & (cv_df.max_proba < rm_point)]['id']\n",
    "    print(u'修改阈值为%f, 修改数据%d/%d个' % (modifi_point, modifi_df.shape[0], X_train.shape[0]))\n",
    "    print(u'删除阈值为%f, 删除数据%d/%d个' % (rm_point, rm_ids.shape[0], X_train.shape[0]))\n",
    "    modifi_df = modifi_df[['id', 'pred']]\n",
    "    modifi_df.columns = ['id', 'label']\n",
    "    train_info = modifi_labels(train_info, modifi_df)\n",
    "    X_train, train_info = remove_ids(X_train, train_info, rm_ids)\n",
    "    \n",
    "    return(X_train, train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试函数，使用remove_and_modifi函数对数据进行删除和修改label后，测试其泛化表现\n",
    "def remove_and_modifi_val(model, X_train, train_info, label_dict, modifi_point = 1, rm_point = 0):\n",
    "    X_train, X_val, train_info, val_info = train_test_split(X_train, train_info, test_size=0.3, random_state=SEED)\n",
    "    if (modifi_point < 1) or (rm_point > 0):\n",
    "        X_train, train_info = remove_and_modifi(X_train, train_info, modifi_point, rm_point)\n",
    "    if model == 'cnn':\n",
    "        X_train, y_train = preproc(X_train, train_info, label_dict, True)\n",
    "        y_preds = cnn_predict(X_train, y_train, X_val, 70)\n",
    "    elif model == 'xgb':\n",
    "        X_train, y_train = preproc(X_train, train_info, label_dict, False)\n",
    "        y_preds = xgb_predict(X_train, y_train, X_val)\n",
    "    val_info['pred'] = postproc(y_preds, label_dict)\n",
    "    wrong_num = val_info[val_info.label != val_info.pred].shape[0]\n",
    "    print(u'\\n在验证集上错误率为%f' % (wrong_num / val_info.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改重复数据前有数据18686个\n",
      "修改重复数据后有数据18673个\n"
     ]
    }
   ],
   "source": [
    "train_info, test_info, label_dict, label_dict_reverse = load_info()\n",
    "X_train, y_train, X_test = load_data()\n",
    "print(u'修改重复数据前有数据%d个' % X_train.shape[0])\n",
    "X_train, train_info = modifi_repeated(X_train, train_info, label_dict, 70)\n",
    "print(u'修改重复数据后有数据%d个' % X_train.shape[0])\n",
    "X_train, train_info = shuffle(X_train, train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改阈值为0.880000, 修改数据754/13071个\n",
      "删除阈值为0.350000, 删除数据200/13071个\n",
      "Train on 10346 samples, validate on 2587 samples\n",
      "Epoch 1/70\n",
      "10346/10346 [==============================] - 0s - loss: 4.3452 - acc: 0.0536 - val_loss: 3.5120 - val_acc: 0.3533\n",
      "Epoch 2/70\n",
      "10346/10346 [==============================] - 0s - loss: 3.4993 - acc: 0.1997 - val_loss: 2.4737 - val_acc: 0.5991\n",
      "Epoch 3/70\n",
      "10346/10346 [==============================] - 0s - loss: 2.6653 - acc: 0.3712 - val_loss: 1.7523 - val_acc: 0.6966\n",
      "Epoch 4/70\n",
      "10346/10346 [==============================] - 0s - loss: 2.0580 - acc: 0.4989 - val_loss: 1.3247 - val_acc: 0.7426\n",
      "Epoch 5/70\n",
      "10346/10346 [==============================] - 0s - loss: 1.6356 - acc: 0.5798 - val_loss: 1.0732 - val_acc: 0.7630\n",
      "Epoch 6/70\n",
      "10346/10346 [==============================] - 0s - loss: 1.3682 - acc: 0.6347 - val_loss: 0.9150 - val_acc: 0.7886\n",
      "Epoch 7/70\n",
      "10346/10346 [==============================] - 0s - loss: 1.1833 - acc: 0.6764 - val_loss: 0.8136 - val_acc: 0.7963\n",
      "Epoch 8/70\n",
      "10346/10346 [==============================] - 0s - loss: 1.0441 - acc: 0.7076 - val_loss: 0.7492 - val_acc: 0.7978\n",
      "Epoch 9/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.9524 - acc: 0.7264 - val_loss: 0.6949 - val_acc: 0.8056\n",
      "Epoch 10/70\n",
      "10346/10346 [==============================] - ETA: 0s - loss: 0.8772 - acc: 0.745 - 0s - loss: 0.8703 - acc: 0.7478 - val_loss: 0.6570 - val_acc: 0.8164\n",
      "Epoch 11/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.8089 - acc: 0.7656 - val_loss: 0.6282 - val_acc: 0.8172\n",
      "Epoch 12/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.7589 - acc: 0.7754 - val_loss: 0.6022 - val_acc: 0.8226\n",
      "Epoch 13/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.7285 - acc: 0.7810 - val_loss: 0.5848 - val_acc: 0.8272\n",
      "Epoch 14/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.6882 - acc: 0.7919 - val_loss: 0.5671 - val_acc: 0.8295\n",
      "Epoch 15/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.6466 - acc: 0.8005 - val_loss: 0.5521 - val_acc: 0.8353\n",
      "Epoch 16/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.6231 - acc: 0.8084 - val_loss: 0.5445 - val_acc: 0.8315\n",
      "Epoch 17/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.5942 - acc: 0.8201 - val_loss: 0.5315 - val_acc: 0.8353\n",
      "Epoch 18/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.5759 - acc: 0.8227 - val_loss: 0.5233 - val_acc: 0.8376\n",
      "Epoch 19/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.5680 - acc: 0.8234 - val_loss: 0.5192 - val_acc: 0.8380\n",
      "Epoch 20/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.5406 - acc: 0.8342 - val_loss: 0.5091 - val_acc: 0.8400\n",
      "Epoch 21/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.5254 - acc: 0.8381 - val_loss: 0.5021 - val_acc: 0.8431\n",
      "Epoch 22/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.5088 - acc: 0.8396 - val_loss: 0.4995 - val_acc: 0.8396\n",
      "Epoch 23/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.5014 - acc: 0.8436 - val_loss: 0.4952 - val_acc: 0.8411\n",
      "Epoch 24/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4795 - acc: 0.8490 - val_loss: 0.4879 - val_acc: 0.8434\n",
      "Epoch 25/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4809 - acc: 0.8467 - val_loss: 0.4845 - val_acc: 0.8477\n",
      "Epoch 26/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4622 - acc: 0.8555 - val_loss: 0.4837 - val_acc: 0.8450\n",
      "Epoch 27/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4560 - acc: 0.8546 - val_loss: 0.4795 - val_acc: 0.8450\n",
      "Epoch 28/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4299 - acc: 0.8653 - val_loss: 0.4760 - val_acc: 0.8462\n",
      "Epoch 29/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4322 - acc: 0.8581 - val_loss: 0.4768 - val_acc: 0.8469\n",
      "Epoch 30/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4143 - acc: 0.8684 - val_loss: 0.4752 - val_acc: 0.8454\n",
      "Epoch 31/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4030 - acc: 0.8710 - val_loss: 0.4725 - val_acc: 0.8446\n",
      "Epoch 32/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.4012 - acc: 0.8725 - val_loss: 0.4718 - val_acc: 0.8504\n",
      "Epoch 33/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3934 - acc: 0.8714 - val_loss: 0.4690 - val_acc: 0.8489\n",
      "Epoch 34/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3869 - acc: 0.8761 - val_loss: 0.4700 - val_acc: 0.8492\n",
      "Epoch 35/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3827 - acc: 0.8772 - val_loss: 0.4653 - val_acc: 0.8485\n",
      "Epoch 36/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3712 - acc: 0.8841 - val_loss: 0.4642 - val_acc: 0.8481\n",
      "Epoch 37/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3719 - acc: 0.8815 - val_loss: 0.4642 - val_acc: 0.8496\n",
      "Epoch 38/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3597 - acc: 0.8881 - val_loss: 0.4646 - val_acc: 0.8504\n",
      "Epoch 39/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3529 - acc: 0.8875 - val_loss: 0.4624 - val_acc: 0.8473\n",
      "Epoch 40/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3507 - acc: 0.8876 - val_loss: 0.4602 - val_acc: 0.8473\n",
      "Epoch 41/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3485 - acc: 0.8898 - val_loss: 0.4611 - val_acc: 0.8527\n",
      "Epoch 42/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3377 - acc: 0.8925 - val_loss: 0.4600 - val_acc: 0.8520\n",
      "Epoch 43/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3328 - acc: 0.8905 - val_loss: 0.4598 - val_acc: 0.8543\n",
      "Epoch 44/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3207 - acc: 0.8973 - val_loss: 0.4586 - val_acc: 0.8492\n",
      "Epoch 45/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3290 - acc: 0.8944 - val_loss: 0.4616 - val_acc: 0.8477\n",
      "Epoch 46/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3209 - acc: 0.8998 - val_loss: 0.4575 - val_acc: 0.8504\n",
      "Epoch 47/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3056 - acc: 0.9004 - val_loss: 0.4557 - val_acc: 0.8523\n",
      "Epoch 48/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3049 - acc: 0.9020 - val_loss: 0.4568 - val_acc: 0.8512\n",
      "Epoch 49/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3050 - acc: 0.9014 - val_loss: 0.4566 - val_acc: 0.8523\n",
      "Epoch 50/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.3009 - acc: 0.9028 - val_loss: 0.4563 - val_acc: 0.8527\n",
      "Epoch 51/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2882 - acc: 0.9063 - val_loss: 0.4550 - val_acc: 0.8496\n",
      "Epoch 52/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2861 - acc: 0.9086 - val_loss: 0.4521 - val_acc: 0.8527\n",
      "Epoch 53/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2836 - acc: 0.9100 - val_loss: 0.4550 - val_acc: 0.8554\n",
      "Epoch 54/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2811 - acc: 0.9090 - val_loss: 0.4590 - val_acc: 0.8543\n",
      "Epoch 55/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2853 - acc: 0.9072 - val_loss: 0.4561 - val_acc: 0.8543\n",
      "Epoch 56/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2783 - acc: 0.9112 - val_loss: 0.4555 - val_acc: 0.8512\n",
      "Epoch 57/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2668 - acc: 0.9135 - val_loss: 0.4533 - val_acc: 0.8550\n",
      "Epoch 58/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2691 - acc: 0.9119 - val_loss: 0.4564 - val_acc: 0.8547\n",
      "Epoch 59/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2673 - acc: 0.9143 - val_loss: 0.4556 - val_acc: 0.8504\n",
      "Epoch 60/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2586 - acc: 0.9150 - val_loss: 0.4538 - val_acc: 0.8543\n",
      "Epoch 61/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2645 - acc: 0.9142 - val_loss: 0.4545 - val_acc: 0.8562\n",
      "Epoch 62/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2486 - acc: 0.9184 - val_loss: 0.4519 - val_acc: 0.8527\n",
      "Epoch 63/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10346/10346 [==============================] - 0s - loss: 0.2472 - acc: 0.9189 - val_loss: 0.4532 - val_acc: 0.8547\n",
      "Epoch 64/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2477 - acc: 0.9203 - val_loss: 0.4537 - val_acc: 0.8527\n",
      "Epoch 65/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2482 - acc: 0.9191 - val_loss: 0.4550 - val_acc: 0.8516\n",
      "Epoch 66/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2377 - acc: 0.9254 - val_loss: 0.4538 - val_acc: 0.8539\n",
      "Epoch 67/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2393 - acc: 0.9219 - val_loss: 0.4560 - val_acc: 0.8535\n",
      "Epoch 68/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2338 - acc: 0.9249 - val_loss: 0.4528 - val_acc: 0.8539\n",
      "Epoch 69/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2277 - acc: 0.9272 - val_loss: 0.4539 - val_acc: 0.8520\n",
      "Epoch 70/70\n",
      "10346/10346 [==============================] - 0s - loss: 0.2364 - acc: 0.9245 - val_loss: 0.4532 - val_acc: 0.8504\n",
      "5280/5602 [===========================>..] - ETA: 0s\n",
      "在验证集上错误率为0.198322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maqiao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# 不断调整modifi_point和rm_point的取值进行交叉训练，\n",
    "# 得出在modifi_point=0.88, rm_point=0.3时，线下得分最高\n",
    "remove_and_modifi_val('cnn', X_train, train_info, label_dict, 0.88, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改阈值为0.880000, 修改数据754/13071个\n",
      "删除阈值为0.300000, 删除数据103/13071个\n",
      "Train on 10399 samples, validate on 2600 samples\n",
      "Epoch 1/70\n",
      "10399/10399 [==============================] - 0s - loss: 4.3680 - acc: 0.0482 - val_loss: 3.5996 - val_acc: 0.3092\n",
      "Epoch 2/70\n",
      "10399/10399 [==============================] - 0s - loss: 3.5529 - acc: 0.1867 - val_loss: 2.5601 - val_acc: 0.5915\n",
      "Epoch 3/70\n",
      "10399/10399 [==============================] - 0s - loss: 2.6870 - acc: 0.3702 - val_loss: 1.8083 - val_acc: 0.6877\n",
      "Epoch 4/70\n",
      "10399/10399 [==============================] - 0s - loss: 2.0709 - acc: 0.5005 - val_loss: 1.3753 - val_acc: 0.7385\n",
      "Epoch 5/70\n",
      "10399/10399 [==============================] - 0s - loss: 1.6521 - acc: 0.5838 - val_loss: 1.1119 - val_acc: 0.7631\n",
      "Epoch 6/70\n",
      "10399/10399 [==============================] - 0s - loss: 1.3822 - acc: 0.6411 - val_loss: 0.9550 - val_acc: 0.7738\n",
      "Epoch 7/70\n",
      "10399/10399 [==============================] - 0s - loss: 1.1772 - acc: 0.6836 - val_loss: 0.8510 - val_acc: 0.7881\n",
      "Epoch 8/70\n",
      "10399/10399 [==============================] - 0s - loss: 1.0550 - acc: 0.7071 - val_loss: 0.7778 - val_acc: 0.7985\n",
      "Epoch 9/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.9638 - acc: 0.7271 - val_loss: 0.7270 - val_acc: 0.8023\n",
      "Epoch 10/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.8874 - acc: 0.7424 - val_loss: 0.6891 - val_acc: 0.8046\n",
      "Epoch 11/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.8118 - acc: 0.7614 - val_loss: 0.6593 - val_acc: 0.8127\n",
      "Epoch 12/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.7709 - acc: 0.7687 - val_loss: 0.6397 - val_acc: 0.8123\n",
      "Epoch 13/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.7277 - acc: 0.7772 - val_loss: 0.6194 - val_acc: 0.8177\n",
      "Epoch 14/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.6818 - acc: 0.7920 - val_loss: 0.6029 - val_acc: 0.8215\n",
      "Epoch 15/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.6639 - acc: 0.7999 - val_loss: 0.5927 - val_acc: 0.8208\n",
      "Epoch 16/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.6222 - acc: 0.8057 - val_loss: 0.5826 - val_acc: 0.8231\n",
      "Epoch 17/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.6053 - acc: 0.8105 - val_loss: 0.5714 - val_acc: 0.8262\n",
      "Epoch 18/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.5807 - acc: 0.8190 - val_loss: 0.5629 - val_acc: 0.8262\n",
      "Epoch 19/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.5588 - acc: 0.8258 - val_loss: 0.5572 - val_acc: 0.8250\n",
      "Epoch 20/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.5430 - acc: 0.8299 - val_loss: 0.5515 - val_acc: 0.8288\n",
      "Epoch 21/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.5295 - acc: 0.8353 - val_loss: 0.5497 - val_acc: 0.8338\n",
      "Epoch 22/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.5089 - acc: 0.8384 - val_loss: 0.5438 - val_acc: 0.8327\n",
      "Epoch 23/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4906 - acc: 0.8439 - val_loss: 0.5402 - val_acc: 0.8354\n",
      "Epoch 24/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4834 - acc: 0.8477 - val_loss: 0.5345 - val_acc: 0.8354\n",
      "Epoch 25/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4766 - acc: 0.8479 - val_loss: 0.5337 - val_acc: 0.8350\n",
      "Epoch 26/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4594 - acc: 0.8553 - val_loss: 0.5330 - val_acc: 0.8369\n",
      "Epoch 27/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4450 - acc: 0.8604 - val_loss: 0.5283 - val_acc: 0.8362\n",
      "Epoch 28/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4408 - acc: 0.8608 - val_loss: 0.5212 - val_acc: 0.8400\n",
      "Epoch 29/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4279 - acc: 0.8633 - val_loss: 0.5240 - val_acc: 0.8373\n",
      "Epoch 30/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4227 - acc: 0.8655 - val_loss: 0.5192 - val_acc: 0.8385\n",
      "Epoch 31/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4165 - acc: 0.8681 - val_loss: 0.5206 - val_acc: 0.8350\n",
      "Epoch 32/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.4020 - acc: 0.8716 - val_loss: 0.5161 - val_acc: 0.8400\n",
      "Epoch 33/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3956 - acc: 0.8734 - val_loss: 0.5201 - val_acc: 0.8396\n",
      "Epoch 34/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3883 - acc: 0.8792 - val_loss: 0.5160 - val_acc: 0.8423\n",
      "Epoch 35/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3805 - acc: 0.8776 - val_loss: 0.5164 - val_acc: 0.8419\n",
      "Epoch 36/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3760 - acc: 0.8803 - val_loss: 0.5138 - val_acc: 0.8427\n",
      "Epoch 37/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3682 - acc: 0.8835 - val_loss: 0.5117 - val_acc: 0.8442\n",
      "Epoch 38/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3592 - acc: 0.8821 - val_loss: 0.5119 - val_acc: 0.8419\n",
      "Epoch 39/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3541 - acc: 0.8841 - val_loss: 0.5138 - val_acc: 0.8419\n",
      "Epoch 40/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3573 - acc: 0.8835 - val_loss: 0.5129 - val_acc: 0.8412\n",
      "Epoch 41/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3395 - acc: 0.8931 - val_loss: 0.5108 - val_acc: 0.8442\n",
      "Epoch 42/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3356 - acc: 0.8923 - val_loss: 0.5104 - val_acc: 0.8427\n",
      "Epoch 43/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3251 - acc: 0.8969 - val_loss: 0.5106 - val_acc: 0.8415\n",
      "Epoch 44/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3248 - acc: 0.8946 - val_loss: 0.5129 - val_acc: 0.8458\n",
      "Epoch 45/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3247 - acc: 0.8942 - val_loss: 0.5091 - val_acc: 0.8454\n",
      "Epoch 46/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3172 - acc: 0.8993 - val_loss: 0.5105 - val_acc: 0.8438\n",
      "Epoch 47/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.3083 - acc: 0.9015 - val_loss: 0.5077 - val_acc: 0.8431\n",
      "Epoch 48/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2970 - acc: 0.9040 - val_loss: 0.5096 - val_acc: 0.8435\n",
      "Epoch 49/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2989 - acc: 0.9017 - val_loss: 0.5064 - val_acc: 0.8446\n",
      "Epoch 50/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2949 - acc: 0.9048 - val_loss: 0.5049 - val_acc: 0.8435\n",
      "Epoch 51/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2897 - acc: 0.9066 - val_loss: 0.5065 - val_acc: 0.8431\n",
      "Epoch 52/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2851 - acc: 0.9085 - val_loss: 0.5061 - val_acc: 0.8450\n",
      "Epoch 53/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2863 - acc: 0.9093 - val_loss: 0.5071 - val_acc: 0.8458\n",
      "Epoch 54/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2833 - acc: 0.9105 - val_loss: 0.5095 - val_acc: 0.8473\n",
      "Epoch 55/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2711 - acc: 0.9115 - val_loss: 0.5088 - val_acc: 0.8473\n",
      "Epoch 56/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2713 - acc: 0.9153 - val_loss: 0.5065 - val_acc: 0.8462\n",
      "Epoch 57/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2647 - acc: 0.9123 - val_loss: 0.5056 - val_acc: 0.8488\n",
      "Epoch 58/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2622 - acc: 0.9150 - val_loss: 0.5087 - val_acc: 0.8458\n",
      "Epoch 59/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2562 - acc: 0.9201 - val_loss: 0.5066 - val_acc: 0.8473\n",
      "Epoch 60/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2629 - acc: 0.9125 - val_loss: 0.5104 - val_acc: 0.8469\n",
      "Epoch 61/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2549 - acc: 0.9181 - val_loss: 0.5105 - val_acc: 0.8450\n",
      "Epoch 62/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2526 - acc: 0.9171 - val_loss: 0.5065 - val_acc: 0.8438\n",
      "Epoch 63/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2423 - acc: 0.9228 - val_loss: 0.5083 - val_acc: 0.8454\n",
      "Epoch 64/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10399/10399 [==============================] - 0s - loss: 0.2419 - acc: 0.9218 - val_loss: 0.5111 - val_acc: 0.8473\n",
      "Epoch 65/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2410 - acc: 0.9220 - val_loss: 0.5090 - val_acc: 0.8473\n",
      "Epoch 66/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2338 - acc: 0.9249 - val_loss: 0.5085 - val_acc: 0.8462\n",
      "Epoch 67/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2336 - acc: 0.9257 - val_loss: 0.5104 - val_acc: 0.8473\n",
      "Epoch 68/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2338 - acc: 0.9262 - val_loss: 0.5119 - val_acc: 0.8465\n",
      "Epoch 69/70\n",
      "10399/10399 [==============================] - ETA: 0s - loss: 0.2260 - acc: 0.928 - 0s - loss: 0.2258 - acc: 0.9287 - val_loss: 0.5105 - val_acc: 0.8435\n",
      "Epoch 70/70\n",
      "10399/10399 [==============================] - 0s - loss: 0.2251 - acc: 0.9292 - val_loss: 0.5100 - val_acc: 0.8473\n",
      "5184/5602 [==========================>...] - ETA: 0s[0]\ttrain-merror:0.642268\teval-merror:0.651538\n",
      "[1]\ttrain-merror:0.407627\teval-merror:0.425385\n",
      "[2]\ttrain-merror:0.304869\teval-merror:0.336667\n",
      "[3]\ttrain-merror:0.249698\teval-merror:0.297692\n",
      "[4]\ttrain-merror:0.209474\teval-merror:0.269744\n",
      "[5]\ttrain-merror:0.178921\teval-merror:0.253333\n",
      "[6]\ttrain-merror:0.156061\teval-merror:0.242308\n",
      "[7]\ttrain-merror:0.141224\teval-merror:0.235897\n",
      "[8]\ttrain-merror:0.126278\teval-merror:0.226923\n",
      "[9]\ttrain-merror:0.115397\teval-merror:0.224872\n",
      "[10]\ttrain-merror:0.102649\teval-merror:0.220769\n",
      "[11]\ttrain-merror:0.094846\teval-merror:0.214103\n",
      "[12]\ttrain-merror:0.085944\teval-merror:0.213333\n",
      "[13]\ttrain-merror:0.080009\teval-merror:0.208462\n",
      "[14]\ttrain-merror:0.073854\teval-merror:0.20641\n",
      "[15]\ttrain-merror:0.069568\teval-merror:0.204615\n",
      "[16]\ttrain-merror:0.062754\teval-merror:0.20359\n",
      "[17]\ttrain-merror:0.057918\teval-merror:0.202308\n",
      "[18]\ttrain-merror:0.052313\teval-merror:0.202308\n",
      "[19]\ttrain-merror:0.049236\teval-merror:0.200769\n",
      "[20]\ttrain-merror:0.047038\teval-merror:0.200256\n",
      "[21]\ttrain-merror:0.04473\teval-merror:0.198205\n",
      "[22]\ttrain-merror:0.041433\teval-merror:0.197179\n",
      "[23]\ttrain-merror:0.038576\teval-merror:0.194359\n",
      "[24]\ttrain-merror:0.035059\teval-merror:0.191795\n",
      "[25]\ttrain-merror:0.032751\teval-merror:0.192564\n",
      "[26]\ttrain-merror:0.030553\teval-merror:0.190256\n",
      "[27]\ttrain-merror:0.027695\teval-merror:0.190256\n",
      "[28]\ttrain-merror:0.026047\teval-merror:0.188974\n",
      "[29]\ttrain-merror:0.024398\teval-merror:0.188974\n",
      "[30]\ttrain-merror:0.02275\teval-merror:0.188974\n",
      "[31]\ttrain-merror:0.020772\teval-merror:0.187179\n",
      "[32]\ttrain-merror:0.019123\teval-merror:0.186667\n",
      "[33]\ttrain-merror:0.018024\teval-merror:0.187692\n",
      "[34]\ttrain-merror:0.017255\teval-merror:0.186923\n",
      "[35]\ttrain-merror:0.016266\teval-merror:0.186667\n",
      "[36]\ttrain-merror:0.015276\teval-merror:0.184615\n",
      "[37]\ttrain-merror:0.014507\teval-merror:0.18359\n",
      "[38]\ttrain-merror:0.013738\teval-merror:0.184359\n",
      "[39]\ttrain-merror:0.012419\teval-merror:0.183846\n",
      "[40]\ttrain-merror:0.01154\teval-merror:0.184103\n",
      "[41]\ttrain-merror:0.01088\teval-merror:0.184615\n",
      "[42]\ttrain-merror:0.009781\teval-merror:0.182821\n",
      "[43]\ttrain-merror:0.009342\teval-merror:0.182308\n",
      "[44]\ttrain-merror:0.009122\teval-merror:0.182051\n",
      "[45]\ttrain-merror:0.009012\teval-merror:0.180256\n",
      "[46]\ttrain-merror:0.008462\teval-merror:0.181282\n",
      "[47]\ttrain-merror:0.008133\teval-merror:0.180513\n",
      "[48]\ttrain-merror:0.007583\teval-merror:0.18\n",
      "[49]\ttrain-merror:0.006924\teval-merror:0.180513\n",
      "[50]\ttrain-merror:0.006045\teval-merror:0.180256\n",
      "[51]\ttrain-merror:0.005495\teval-merror:0.180256\n",
      "[52]\ttrain-merror:0.005385\teval-merror:0.18\n",
      "[53]\ttrain-merror:0.004836\teval-merror:0.179744\n",
      "[54]\ttrain-merror:0.004616\teval-merror:0.179231\n",
      "[55]\ttrain-merror:0.004286\teval-merror:0.18\n",
      "[56]\ttrain-merror:0.003956\teval-merror:0.179231\n",
      "[57]\ttrain-merror:0.003627\teval-merror:0.179487\n",
      "[58]\ttrain-merror:0.003517\teval-merror:0.179231\n",
      "[59]\ttrain-merror:0.003407\teval-merror:0.18\n",
      "[60]\ttrain-merror:0.003187\teval-merror:0.179487\n",
      "[61]\ttrain-merror:0.003077\teval-merror:0.180256\n",
      "[62]\ttrain-merror:0.002967\teval-merror:0.178462\n",
      "[63]\ttrain-merror:0.002638\teval-merror:0.178205\n",
      "[64]\ttrain-merror:0.002418\teval-merror:0.178205\n",
      "[65]\ttrain-merror:0.002308\teval-merror:0.178205\n",
      "[66]\ttrain-merror:0.002308\teval-merror:0.178718\n",
      "[67]\ttrain-merror:0.002308\teval-merror:0.18\n",
      "[68]\ttrain-merror:0.002198\teval-merror:0.178718\n",
      "[69]\ttrain-merror:0.002198\teval-merror:0.178718\n",
      "[70]\ttrain-merror:0.002088\teval-merror:0.178462\n",
      "[71]\ttrain-merror:0.002088\teval-merror:0.178205\n",
      "[72]\ttrain-merror:0.002088\teval-merror:0.178205\n",
      "[73]\ttrain-merror:0.002088\teval-merror:0.177692\n",
      "[74]\ttrain-merror:0.001868\teval-merror:0.177692\n",
      "[75]\ttrain-merror:0.001758\teval-merror:0.177692\n",
      "[76]\ttrain-merror:0.001649\teval-merror:0.177436\n",
      "[77]\ttrain-merror:0.001649\teval-merror:0.177436\n",
      "[78]\ttrain-merror:0.001649\teval-merror:0.176667\n",
      "[79]\ttrain-merror:0.001539\teval-merror:0.176923\n",
      "[80]\ttrain-merror:0.001319\teval-merror:0.176154\n",
      "[81]\ttrain-merror:0.001319\teval-merror:0.176667\n",
      "[82]\ttrain-merror:0.001429\teval-merror:0.176667\n",
      "[83]\ttrain-merror:0.001429\teval-merror:0.177179\n",
      "[84]\ttrain-merror:0.001319\teval-merror:0.177179\n",
      "[85]\ttrain-merror:0.001319\teval-merror:0.17641\n",
      "[86]\ttrain-merror:0.001209\teval-merror:0.176667\n",
      "[87]\ttrain-merror:0.001209\teval-merror:0.176154\n",
      "[88]\ttrain-merror:0.001209\teval-merror:0.175897\n",
      "[89]\ttrain-merror:0.001209\teval-merror:0.17641\n",
      "[90]\ttrain-merror:0.001209\teval-merror:0.176667\n",
      "[91]\ttrain-merror:0.001209\teval-merror:0.177179\n",
      "[92]\ttrain-merror:0.001209\teval-merror:0.176923\n",
      "[93]\ttrain-merror:0.001099\teval-merror:0.176667\n",
      "[94]\ttrain-merror:0.001099\teval-merror:0.176923\n",
      "[95]\ttrain-merror:0.001099\teval-merror:0.17641\n",
      "[96]\ttrain-merror:0.001099\teval-merror:0.176154\n",
      "[97]\ttrain-merror:0.001099\teval-merror:0.17641\n",
      "[98]\ttrain-merror:0.001099\teval-merror:0.176667\n",
      "[99]\ttrain-merror:0.000989\teval-merror:0.17641\n",
      "\n",
      "在验证集上错误率为0.192967\n"
     ]
    }
   ],
   "source": [
    "# 将提取后的特征向量，使用全连接层训练、和xgboost训练后进行融合\n",
    "X_train, X_val, train_info, val_info = train_test_split(X_train, train_info, test_size=0.3, random_state=SEED)\n",
    "X_train, train_info = remove_and_modifi(X_train, train_info, modifi_point = 0.88, rm_point = 0.3)\n",
    "\n",
    "X_train1, y_train1 = preproc(X_train, train_info, label_dict, True)\n",
    "X_train2, y_train2 = preproc(X_train, train_info, label_dict, False)\n",
    "\n",
    "pred1 = cnn_predict(X_train1, y_train1, X_val, 70)\n",
    "pred2 = xgb_predict(X_train2, y_train2, X_val)\n",
    "\n",
    "pred3 = pred1.copy()\n",
    "for i in range(len(pred1)):\n",
    "    total = sum(pred1[i])\n",
    "    ys = pred1[i] / total\n",
    "    pred3[i] = ys\n",
    "\n",
    "preds = pred3*0.55 + pred2*0.45\n",
    "val_info['pred'] = postproc(preds, label_dict)\n",
    "wrong_num = val_info[val_info.label != val_info.pred].shape[0]\n",
    "print(u'\\n在验证集上错误率为%f' % (wrong_num / val_info.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改阈值为0.880000, 修改数据754/18673个\n",
      "删除阈值为0.300000, 删除数据103/18673个\n",
      "Train on 14856 samples, validate on 3714 samples\n",
      "Epoch 1/70\n",
      "14856/14856 [==============================] - 0s - loss: 4.2405 - acc: 0.0740 - val_loss: 3.1390 - val_acc: 0.4639\n",
      "Epoch 2/70\n",
      "14856/14856 [==============================] - 0s - loss: 3.0408 - acc: 0.2929 - val_loss: 1.8945 - val_acc: 0.6753\n",
      "Epoch 3/70\n",
      "14856/14856 [==============================] - 0s - loss: 2.0966 - acc: 0.4859 - val_loss: 1.2856 - val_acc: 0.7412\n",
      "Epoch 4/70\n",
      "14856/14856 [==============================] - 0s - loss: 1.5549 - acc: 0.5990 - val_loss: 0.9901 - val_acc: 0.7763\n",
      "Epoch 5/70\n",
      "14856/14856 [==============================] - 0s - loss: 1.2585 - acc: 0.6583 - val_loss: 0.8322 - val_acc: 0.7946\n",
      "Epoch 6/70\n",
      "14856/14856 [==============================] - 0s - loss: 1.0720 - acc: 0.6997 - val_loss: 0.7370 - val_acc: 0.8018\n",
      "Epoch 7/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.9439 - acc: 0.7277 - val_loss: 0.6771 - val_acc: 0.8123\n",
      "Epoch 8/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.8528 - acc: 0.7458 - val_loss: 0.6334 - val_acc: 0.8180\n",
      "Epoch 9/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.7825 - acc: 0.7617 - val_loss: 0.6045 - val_acc: 0.8196\n",
      "Epoch 10/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.7354 - acc: 0.7749 - val_loss: 0.5784 - val_acc: 0.8269\n",
      "Epoch 11/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.6936 - acc: 0.7884 - val_loss: 0.5584 - val_acc: 0.8333\n",
      "Epoch 12/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.6611 - acc: 0.7975 - val_loss: 0.5434 - val_acc: 0.8339\n",
      "Epoch 13/70\n",
      "14856/14856 [==============================] - ETA: 0s - loss: 0.6401 - acc: 0.798 - 0s - loss: 0.6402 - acc: 0.7984 - val_loss: 0.5337 - val_acc: 0.8347\n",
      "Epoch 14/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.6108 - acc: 0.8053 - val_loss: 0.5218 - val_acc: 0.8395\n",
      "Epoch 15/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.5776 - acc: 0.8178 - val_loss: 0.5131 - val_acc: 0.8422\n",
      "Epoch 16/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.5598 - acc: 0.8265 - val_loss: 0.5066 - val_acc: 0.8395\n",
      "Epoch 17/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.5437 - acc: 0.8273 - val_loss: 0.5034 - val_acc: 0.8414\n",
      "Epoch 18/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.5285 - acc: 0.8315 - val_loss: 0.4953 - val_acc: 0.8409\n",
      "Epoch 19/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.5105 - acc: 0.8384 - val_loss: 0.4919 - val_acc: 0.8452\n",
      "Epoch 20/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4944 - acc: 0.8441 - val_loss: 0.4898 - val_acc: 0.8387\n",
      "Epoch 21/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4886 - acc: 0.8439 - val_loss: 0.4823 - val_acc: 0.8473\n",
      "Epoch 22/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4741 - acc: 0.8500 - val_loss: 0.4812 - val_acc: 0.8468\n",
      "Epoch 23/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4599 - acc: 0.8549 - val_loss: 0.4802 - val_acc: 0.8438\n",
      "Epoch 24/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4517 - acc: 0.8578 - val_loss: 0.4750 - val_acc: 0.8500\n",
      "Epoch 25/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4412 - acc: 0.8598 - val_loss: 0.4723 - val_acc: 0.8498\n",
      "Epoch 26/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4334 - acc: 0.8596 - val_loss: 0.4700 - val_acc: 0.8498\n",
      "Epoch 27/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4263 - acc: 0.8638 - val_loss: 0.4690 - val_acc: 0.8500\n",
      "Epoch 28/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4147 - acc: 0.8645 - val_loss: 0.4676 - val_acc: 0.8495\n",
      "Epoch 29/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4094 - acc: 0.8667 - val_loss: 0.4666 - val_acc: 0.8506\n",
      "Epoch 30/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4052 - acc: 0.8714 - val_loss: 0.4647 - val_acc: 0.8525\n",
      "Epoch 31/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.4020 - acc: 0.8714 - val_loss: 0.4637 - val_acc: 0.8503\n",
      "Epoch 32/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3785 - acc: 0.8764 - val_loss: 0.4619 - val_acc: 0.8508\n",
      "Epoch 33/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3743 - acc: 0.8778 - val_loss: 0.4604 - val_acc: 0.8551\n",
      "Epoch 34/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3786 - acc: 0.8803 - val_loss: 0.4605 - val_acc: 0.8484\n",
      "Epoch 35/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3747 - acc: 0.8778 - val_loss: 0.4613 - val_acc: 0.8498\n",
      "Epoch 36/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3636 - acc: 0.8823 - val_loss: 0.4603 - val_acc: 0.8498\n",
      "Epoch 37/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3568 - acc: 0.8856 - val_loss: 0.4585 - val_acc: 0.8527\n",
      "Epoch 38/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3512 - acc: 0.8856 - val_loss: 0.4569 - val_acc: 0.8514\n",
      "Epoch 39/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3492 - acc: 0.8867 - val_loss: 0.4569 - val_acc: 0.8527\n",
      "Epoch 40/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3380 - acc: 0.8919 - val_loss: 0.4569 - val_acc: 0.8525\n",
      "Epoch 41/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3384 - acc: 0.8899 - val_loss: 0.4599 - val_acc: 0.8543\n",
      "Epoch 42/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3287 - acc: 0.8978 - val_loss: 0.4568 - val_acc: 0.8522\n",
      "Epoch 43/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3281 - acc: 0.8953 - val_loss: 0.4563 - val_acc: 0.8533\n",
      "Epoch 44/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3219 - acc: 0.8936 - val_loss: 0.4548 - val_acc: 0.8562\n",
      "Epoch 45/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3181 - acc: 0.8961 - val_loss: 0.4562 - val_acc: 0.8560\n",
      "Epoch 46/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3146 - acc: 0.9005 - val_loss: 0.4521 - val_acc: 0.8576\n",
      "Epoch 47/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3129 - acc: 0.8971 - val_loss: 0.4580 - val_acc: 0.8543\n",
      "Epoch 48/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3042 - acc: 0.9002 - val_loss: 0.4544 - val_acc: 0.8527\n",
      "Epoch 49/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.3037 - acc: 0.8998 - val_loss: 0.4569 - val_acc: 0.8551\n",
      "Epoch 50/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2983 - acc: 0.9047 - val_loss: 0.4556 - val_acc: 0.8535\n",
      "Epoch 51/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2964 - acc: 0.9009 - val_loss: 0.4562 - val_acc: 0.8543\n",
      "Epoch 52/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2876 - acc: 0.9062 - val_loss: 0.4562 - val_acc: 0.8516\n",
      "Epoch 53/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2829 - acc: 0.9060 - val_loss: 0.4551 - val_acc: 0.8560\n",
      "Epoch 54/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2923 - acc: 0.9060 - val_loss: 0.4556 - val_acc: 0.8549\n",
      "Epoch 55/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2806 - acc: 0.9060 - val_loss: 0.4568 - val_acc: 0.8562\n",
      "Epoch 56/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2769 - acc: 0.9091 - val_loss: 0.4543 - val_acc: 0.8570\n",
      "Epoch 57/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2703 - acc: 0.9125 - val_loss: 0.4523 - val_acc: 0.8554\n",
      "Epoch 58/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2716 - acc: 0.9091 - val_loss: 0.4572 - val_acc: 0.8538\n",
      "Epoch 59/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2708 - acc: 0.9116 - val_loss: 0.4537 - val_acc: 0.8562\n",
      "Epoch 60/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2589 - acc: 0.9141 - val_loss: 0.4537 - val_acc: 0.8560\n",
      "Epoch 61/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2603 - acc: 0.9155 - val_loss: 0.4565 - val_acc: 0.8554\n",
      "Epoch 62/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2557 - acc: 0.9171 - val_loss: 0.4576 - val_acc: 0.8551\n",
      "Epoch 63/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14856/14856 [==============================] - 0s - loss: 0.2573 - acc: 0.9157 - val_loss: 0.4566 - val_acc: 0.8543\n",
      "Epoch 64/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2505 - acc: 0.9202 - val_loss: 0.4568 - val_acc: 0.8565\n",
      "Epoch 65/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2458 - acc: 0.9179 - val_loss: 0.4574 - val_acc: 0.8595\n",
      "Epoch 66/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2492 - acc: 0.9198 - val_loss: 0.4557 - val_acc: 0.8557\n",
      "Epoch 67/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2432 - acc: 0.9198 - val_loss: 0.4559 - val_acc: 0.8565\n",
      "Epoch 68/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2418 - acc: 0.9205 - val_loss: 0.4567 - val_acc: 0.8565\n",
      "Epoch 69/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2406 - acc: 0.9207 - val_loss: 0.4582 - val_acc: 0.8543\n",
      "Epoch 70/70\n",
      "14856/14856 [==============================] - 0s - loss: 0.2378 - acc: 0.9212 - val_loss: 0.4571 - val_acc: 0.8581\n",
      "10400/10593 [============================>.] - ETA: 0s[0]\ttrain-merror:0.534041\teval-merror:0.541554\n",
      "[1]\ttrain-merror:0.368259\teval-merror:0.39221\n",
      "[2]\ttrain-merror:0.287791\teval-merror:0.332795\n",
      "[3]\ttrain-merror:0.238018\teval-merror:0.293664\n",
      "[4]\ttrain-merror:0.199477\teval-merror:0.274098\n",
      "[5]\ttrain-merror:0.171859\teval-merror:0.258122\n",
      "[6]\ttrain-merror:0.151242\teval-merror:0.247532\n",
      "[7]\ttrain-merror:0.135395\teval-merror:0.236582\n",
      "[8]\ttrain-merror:0.123009\teval-merror:0.2303\n",
      "[9]\ttrain-merror:0.112778\teval-merror:0.223299\n",
      "[10]\ttrain-merror:0.102316\teval-merror:0.219171\n",
      "[11]\ttrain-merror:0.09293\teval-merror:0.215581\n",
      "[12]\ttrain-merror:0.08693\teval-merror:0.209478\n",
      "[13]\ttrain-merror:0.080314\teval-merror:0.20858\n",
      "[14]\ttrain-merror:0.074467\teval-merror:0.205529\n",
      "[15]\ttrain-merror:0.067621\teval-merror:0.203016\n",
      "[16]\ttrain-merror:0.064005\teval-merror:0.204452\n",
      "[17]\ttrain-merror:0.059158\teval-merror:0.20158\n",
      "[18]\ttrain-merror:0.055927\teval-merror:0.200682\n",
      "[19]\ttrain-merror:0.051927\teval-merror:0.198349\n",
      "[20]\ttrain-merror:0.04785\teval-merror:0.197272\n",
      "[21]\ttrain-merror:0.045157\teval-merror:0.197451\n",
      "[22]\ttrain-merror:0.041849\teval-merror:0.196374\n",
      "[23]\ttrain-merror:0.038157\teval-merror:0.194938\n",
      "[24]\ttrain-merror:0.036695\teval-merror:0.193861\n",
      "[25]\ttrain-merror:0.033695\teval-merror:0.192246\n",
      "[26]\ttrain-merror:0.031002\teval-merror:0.191528\n",
      "[27]\ttrain-merror:0.029233\teval-merror:0.191169\n",
      "[28]\ttrain-merror:0.027002\teval-merror:0.189912\n",
      "[29]\ttrain-merror:0.025079\teval-merror:0.188297\n",
      "[30]\ttrain-merror:0.023002\teval-merror:0.187938\n",
      "[31]\ttrain-merror:0.020694\teval-merror:0.187938\n",
      "[32]\ttrain-merror:0.019694\teval-merror:0.186861\n",
      "[33]\ttrain-merror:0.018001\teval-merror:0.185963\n",
      "[34]\ttrain-merror:0.01654\teval-merror:0.185066\n",
      "[35]\ttrain-merror:0.01577\teval-merror:0.184348\n",
      "[36]\ttrain-merror:0.014847\teval-merror:0.182014\n",
      "[37]\ttrain-merror:0.013693\teval-merror:0.182912\n",
      "[38]\ttrain-merror:0.012462\teval-merror:0.182912\n",
      "[39]\ttrain-merror:0.011616\teval-merror:0.182373\n",
      "[40]\ttrain-merror:0.010539\teval-merror:0.180757\n",
      "[41]\ttrain-merror:0.010001\teval-merror:0.180219\n",
      "[42]\ttrain-merror:0.009385\teval-merror:0.180219\n",
      "[43]\ttrain-merror:0.008616\teval-merror:0.180039\n",
      "[44]\ttrain-merror:0.008154\teval-merror:0.178783\n",
      "[45]\ttrain-merror:0.007001\teval-merror:0.179501\n",
      "[46]\ttrain-merror:0.006616\teval-merror:0.180039\n",
      "[47]\ttrain-merror:0.006154\teval-merror:0.178962\n",
      "[48]\ttrain-merror:0.005924\teval-merror:0.178783\n",
      "[49]\ttrain-merror:0.005616\teval-merror:0.179142\n",
      "[50]\ttrain-merror:0.005077\teval-merror:0.179501\n",
      "[51]\ttrain-merror:0.004923\teval-merror:0.180219\n",
      "[52]\ttrain-merror:0.004616\teval-merror:0.179321\n",
      "[53]\ttrain-merror:0.004077\teval-merror:0.178962\n",
      "[54]\ttrain-merror:0.003846\teval-merror:0.178424\n",
      "[55]\ttrain-merror:0.003539\teval-merror:0.178244\n",
      "[56]\ttrain-merror:0.003308\teval-merror:0.177526\n",
      "[57]\ttrain-merror:0.003231\teval-merror:0.176808\n",
      "[58]\ttrain-merror:0.002923\teval-merror:0.17627\n",
      "[59]\ttrain-merror:0.002769\teval-merror:0.177167\n",
      "[60]\ttrain-merror:0.002539\teval-merror:0.176988\n",
      "[61]\ttrain-merror:0.002462\teval-merror:0.176629\n",
      "[62]\ttrain-merror:0.002385\teval-merror:0.177347\n",
      "[63]\ttrain-merror:0.002077\teval-merror:0.176988\n",
      "[64]\ttrain-merror:0.002\teval-merror:0.177885\n",
      "[65]\ttrain-merror:0.001923\teval-merror:0.178783\n",
      "[66]\ttrain-merror:0.001846\teval-merror:0.178065\n",
      "[67]\ttrain-merror:0.001769\teval-merror:0.176629\n",
      "[68]\ttrain-merror:0.001616\teval-merror:0.17627\n",
      "[69]\ttrain-merror:0.001616\teval-merror:0.176988\n",
      "[70]\ttrain-merror:0.001462\teval-merror:0.177526\n",
      "[71]\ttrain-merror:0.001462\teval-merror:0.17627\n",
      "[72]\ttrain-merror:0.001462\teval-merror:0.176449\n",
      "[73]\ttrain-merror:0.001462\teval-merror:0.17627\n",
      "[74]\ttrain-merror:0.001385\teval-merror:0.17627\n",
      "[75]\ttrain-merror:0.001385\teval-merror:0.175731\n",
      "[76]\ttrain-merror:0.001308\teval-merror:0.175731\n",
      "[77]\ttrain-merror:0.001308\teval-merror:0.174295\n",
      "[78]\ttrain-merror:0.001308\teval-merror:0.174295\n",
      "[79]\ttrain-merror:0.001308\teval-merror:0.173757\n",
      "[80]\ttrain-merror:0.001154\teval-merror:0.174834\n",
      "[81]\ttrain-merror:0.001\teval-merror:0.174834\n",
      "[82]\ttrain-merror:0.000923\teval-merror:0.174654\n",
      "[83]\ttrain-merror:0.000846\teval-merror:0.174654\n",
      "[84]\ttrain-merror:0.000846\teval-merror:0.174116\n",
      "[85]\ttrain-merror:0.000846\teval-merror:0.174475\n",
      "[86]\ttrain-merror:0.000846\teval-merror:0.173218\n",
      "[87]\ttrain-merror:0.000846\teval-merror:0.173398\n",
      "[88]\ttrain-merror:0.000846\teval-merror:0.173936\n",
      "[89]\ttrain-merror:0.000846\teval-merror:0.173757\n",
      "[90]\ttrain-merror:0.000615\teval-merror:0.174295\n",
      "[91]\ttrain-merror:0.000615\teval-merror:0.173577\n",
      "[92]\ttrain-merror:0.000615\teval-merror:0.173398\n",
      "[93]\ttrain-merror:0.000615\teval-merror:0.173218\n",
      "[94]\ttrain-merror:0.000615\teval-merror:0.173218\n",
      "[95]\ttrain-merror:0.000539\teval-merror:0.17268\n",
      "[96]\ttrain-merror:0.000539\teval-merror:0.172859\n",
      "[97]\ttrain-merror:0.000615\teval-merror:0.1725\n",
      "[98]\ttrain-merror:0.000615\teval-merror:0.172321\n",
      "[99]\ttrain-merror:0.000615\teval-merror:0.171962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1705966597,1055292353</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2473192235,4122785580</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4188013342,2932602063</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1022790570,2912884051</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1031491946,2813127030</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2357588397,2237003912</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1443728590,2022328737</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2895133363,3781516925</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1551966137,2044502403</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1573069851,1977047833</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  label\n",
       "0  1705966597,1055292353     36\n",
       "1  2473192235,4122785580     45\n",
       "2  4188013342,2932602063     78\n",
       "3  1022790570,2912884051     26\n",
       "4  1031491946,2813127030     34\n",
       "5  2357588397,2237003912     88\n",
       "6  1443728590,2022328737     67\n",
       "7  2895133363,3781516925     68\n",
       "8  1551966137,2044502403     38\n",
       "9  1573069851,1977047833     42"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将提取后的特征向量，使用全连接层训练、和xgboost训练后进行融合\n",
    "X_train, train_info = remove_and_modifi(X_train, train_info, modifi_point = 0.88, rm_point = 0.3)\n",
    "\n",
    "X_train1, y_train1 = preproc(X_train, train_info, label_dict, True)\n",
    "X_train2, y_train2 = preproc(X_train, train_info, label_dict, False)\n",
    "\n",
    "pred1 = cnn_predict(X_train1, y_train1, X_test, 70)\n",
    "pred2 = xgb_predict(X_train2, y_train2, X_test)\n",
    "\n",
    "pred3 = pred1.copy()\n",
    "for i in range(len(pred1)):\n",
    "    total = sum(pred1[i])\n",
    "    ys = pred1[i] / total\n",
    "    pred3[i] = ys\n",
    "\n",
    "preds = pred3*0.55 + pred2*0.45\n",
    "preds = postproc(preds, label_dict)\n",
    "submission = pd.DataFrame({'label': preds, 'id':test_info.id})\n",
    "\n",
    "from datetime import datetime\n",
    "submission.to_csv('sub/sub_%s_3.txt' % datetime.now().strftime('%Y-%m-%d'),\n",
    "                  sep='\\t', columns=['label', 'id'], header=False, index=False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def rmrf_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)\n",
    "def create_wrong_classify(train_df, predict_df):\n",
    "    files_dir = 'data/wrong_classify/train/'\n",
    "    rmrf_mkdir(files_dir)\n",
    "    label_name = pd.read_table('data/label_name.txt', header=None, sep=' ', na_filter=False)\n",
    "    label_name.columns = ['name', 'label']\n",
    "    label_name['name'] = label_name['name'].apply(lambda x: x.split('---')[-1])\\\n",
    "                                           .apply(lambda x: x.split('|')[0]).apply(lambda x: x.split('/')[0])\n",
    "    label_set = set(train_df.label)\n",
    "    remains = label_name.label.isin(label_set)\n",
    "    label_name = label_name[remains]\n",
    "    for i, x in label_name.iterrows():\n",
    "        target_dir = files_dir + str(x['label']) + '_' + x['name']\n",
    "        os.mkdir(target_dir)\n",
    "        os.mkdir(target_dir + '/wrong')\n",
    "    train_df = pd.merge(train_df, label_name, on='label', how='left')\n",
    "    train_df = pd.merge(train_df, predict_df[['id', 'pred', 'max_proba']], on='id', how='left')\n",
    "    for i, x in train_df.iterrows():\n",
    "        target_dir = files_dir + str(x['label']) + '_' + x['name'] + '/'\n",
    "        shutil.copyfile('data/alldata/' + x.id + '.jpg',\n",
    "                         target_dir + x.id + '.jpg')\n",
    "        if x.label != x.pred:\n",
    "            shutil.move(target_dir + x.id + '.jpg',\n",
    "                            target_dir + 'wrong/' + x.id + '_' + str(int(x.pred)) + '_' + '%.2e'%x.max_proba + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create_wrong_classify函数为辅助函数，将预测错误的图片分在不同的文件夹，方便手动寻找预测错误的规律\n",
    "cv_df = cross_predict()\n",
    "create_wrong_classify(train_info, cv_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
